---
layout: post
title: "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
description: "Yonghui Wu et al. (Technical Report 2016)"
date: 2020-11-29
categories: paper
tags: [review, deeplearning, machine translation]
image: 
---

# [1] 아이디어 제안 배경

## Neural Machine Translation
신경망 기반 번역(Neural Machine Translation)은 phrase 단위로 분리하여 번역 후 적절한 튜닝을 거치는 이전의 방법론 대신, 입출력 데이터 쌍을 이용한 거대한 단일 신경망 모델로 번역을 수행합니다. 이는 end-to-end로 적용할 수 있어 간편하고, 전반적으로 뛰어난 성능을 보입니다.

NMT의 대표적인 구조는 인코더-디코더 구조입니다. 입력 문장의 각 토큰을 읽은 후 고정된 크기의 벡터(bottleneck)로 변환시키는 인코더와 주어진 벡터로부터 번역 결과물을 출력하는 디코더로 구성됩니다. 다만 모델 구조 상 입력 문장의 모든 정보는 인코더의 마지막 벡터에 압축됩니다. 벡터의 크기는 고정되어 있기 때문에 입력 문장의 길이가 길어질수록 정보를 압축하는 데 한계(long-term dependency)가 존재하여 성능이 떨어지는 문제가 발생하는데, 이 문제를 해결하기 위해 다양한 방법론이 제안되었습니다. 대표적으로는 Cell State가 추가된 LSTM 구조, gradient clipping 등이 있습니다. 하지만 여전히 long-term dependency에 대한 완벽한 해결책은 아니었습니다.

## Attention
그렇게 제안된 방법론이 바로 attention입니다. Attention을 한 마디로 표현하자면, "번역된 문장을 출력하는 매 time step마다 인코더의 모든 hidden state를 참고하여 관련된 정보를 추출"하는 방법론입니다. 따라서 입력 문장의 모든 정보를 고정된 크기의 벡터에 압축시켜야 하는 부담을 덜어 줍니다. 현재는 몇 가지 형태의 attention이 존재하지만, 그 중 대표적인 Bahdanau Attention에 대해 설명드리겠습니다. 먼저 전체적인 그림으로 나타내면 아래와 같습니다. 


<img src="/assets/figures/gnmt_attention.png" width="70%">


디코더의 매 time step에서 번역 단어를 출력할 때마다 이전 time step의 디코더 hidden state와 인코더의 모든 hidden state에 fully-connected layer를 통과시킵니다. 그림에 따르면 인코더의 모든 hidden state에 대해 첫 번째 fc layer를, 이전 time step의 디코더 hidden state에 두 번째 fc layer를, 마지막으로 이 둘을 합친 값에 fc layer를 통과시킨 후 softmax를 적용하여 최종 attention distribution을 만들어냅니다. 


Attention distribution은 ${t }$ 시점의 번역 결과를 출력할 때 인코더의 어떤 부분에 집중해야 하는지에 대한 가중치입니다. 따라서 인코더의 hidden state에 attention distribution을 가중합하여 context vector를 만들어 내고, context vector와 이전 시점의 출력값을 연결하여 디코더의 순환신경망의 입력으로 사용합니다.


## Inherent weaknesses
그러나 본 논문에서는 한창 인기를 끌고 있는 신경망 기반의 기계번역이 실제로는 다음과 같은 문제점을 갖고 있다고 지적합니다.

- 대규모 데이터셋을 학습할 때 연산 비용이 너무 많이 듦
- 학습과 추론 속도가 느림
- 희소 단어 처리에 대한 비효율성
    - 이를 보완하기 위해 attention이 사용되기도 하지만, 규모가 커지면 신뢰할 수 없고 언어 간 alignment에 따라 품질 이슈 발생하며 깊은 신경망에서는 불안정함
- 입력 문장 중 일부를 번역하지 않는 경우가 발생

# [2] 방법론

본 논문은 구글의 번역 시스템 모델(Google NMT;GNMT)을 제안합니다. 먼저 논문에 삽입된 모델 구조는 아래와 같습니다.

<img src="/assets/figures/gnmt_arc.png" width="70%">


전체적인 구조는 위의 Seq2Seq+Attention과 유사하지만 실제 서비스에 적용하는 목적을 갖고 있기 때문에 경험적인, 그리고 실용적인 기술을 많이 추가하였습니다. 각 방법론을 소개하기 전에 위의 방법들을 사용하였을 때 얻을 수 있는 효과에 대해 아래에 미리 적어두겠습니다.

- 각 언어의 특징에 맞는 별도의 처리 없이도 다양한 쌍의 언어에 대해 robust하고 좋은 번역 성능을 보임
- 기존 구글의 phrase-based 번역 시스템보다 훨씬 좋은 성능을 보임 
- 사람에 의한 정성적인 평가에서도 좋은 반응을 얻음

## 1. 모델에 적용된 기술

### Encoder: Residual Connections
우선 구글에서는 Seq2Seq 모델 구조에서는 사용하는 LSTM이 깊을수록 source/target 언어 간의 미묘한 차이를 잘 포착할 수 있는 것을 발견했다고 합니다. 하지만 단순히 층을 많이 쌓는 것은 너무 느리고 학습하기 어려운 문제가 있습니다. 4층까지 쌓은 LSTM은 괜찮게 동작하지만, 6개 그리고 8개까지 늘릴수록 점점 성능이 하락했다고 합니다. 따라서 LSTM의 다음 층으로 넘어갈 때 residual connection(이전의 state를 그대로 더함)을 추가하여 학습 시 gradient가 충분히 반영될 수 있도록 하였습니다.


### Encoder: Bi-directional Encoder for First Layer
일반적으로 인코더에 사용되는 LSTM은 bidirectional하게 구성하는 경우가 많습니다. 디코더와는 다르게 입력 문장 전체에 대한 정보를 갖고 있기 때문이죠. 이는 문장의 context를 더 풍부하게 반영할 수 있는 장점이 있습니다. 그러나 본 논문에서는 가장 인코더 LSTM의 가장 아래 층에만 bidirectional한 구조를 사용합니다. 빠른 학습과 추론을 위해서는 병렬처리가 필수적인데, 병렬처리를 최대화하기 위하여 나머지 층에는 적용하지 않습니다. 

### Decoder: Length Normalization, Coverage Penalty
기본적으로 디코딩 시에는 몇 개의 후보를 함께 출력하여 가장 높은 score를 갖는 조합을 선택하는 beam search를 사용합니다. 다만 문장의 길이가 길어질수록 negative log-probability가 매 step마다 추가되기 때문에 모델이 짧은 문장을 선호하는 현상이 발생합니다. 이 문제를 해결하기 위해 length normalization 기법이 사용됩니다. 


Length normalization을 위한 몇 가지 시도 끝에, 최종적으로는 아래와 같은 방법으로 normalization을 진행하였습니다. $P(Y\mid X)$는 확률값이기 때문에 0과 1 사이의 값을 갖습니다. 따라서 $logP(Y \mid X)$는 음수값을 갖습니다. 따라서 긴 문장일수록 $lp(Y)$의 값이 커지고, score function $s(Y,X)$의 분모에 추가되어 negative log probability의 값이 작아지기 때문에 결과적으로는 score의 손실이 덜해지는 것입니다.

$${ s(Y,X) = log(P(Y \mid X)) / lp(Y) + cp(X;Y) }$$

$${ lp(Y) = { (5+\lvert Y \rvert)^\alpha \over (5+1)^\alpha } }$$

$${ cp(X;Y) = \beta \ast \sum_{i=1}^{\lvert X \rvert} log(min(\sum_{j=1}^{\lvert Y \rvert} p_{i,j}, 1.0)) }$$

여기서 $Y$는 모델이 출력한 번역문 시퀀스, $X$는 입력 시퀀스이고, $s(Y,X)$는 두 문장에 대한 score function입니다. 학습의 목적은 score function을 최대로 하는 $Y$를 찾는 것입니다. $\alpha$와 $\beta$는 각각 length normalization과 coverage penalty를 조절하는 역할을 합니다. 두 값이 0이면 일반적인 beam search와 동일한 형태입니다.


Length normalization에서 coverage penalty의 역할은 attention에 따라 입력 문장의 내용을 완전히 포함(cover)하는 번역문을 선호하도록 하는 것입니다. $ p_{i,j}$는 $i$번째 입력 단어에 대한 $j$번재 출력 단어의 attention 확률입니다. Coverage penalty의 식을 해석하면 아래와 같습니다. 입력 시퀀스의 첫 번째 단어($i=1$)와 두 번째 단어($i=2$)를 예시로 설명하도록 하겠습니다.


<b> [ 첫 번째 단어: 번역문에서 cover하는 경우 ] </b> <br>
- 입력 시퀀스의 첫 번째 단어($x_1$)와 출력 시퀀스의 모든 단어에 대한 attention 확률($p_{1,j}$)을 계산한다.
- 번역문의 특정 위치에서 attention weight가 높게 할당되어 $ min(\sum_{j=1}^{\lvert Y \rvert} p_{i,j}, 1.0) =1$이 되고, $log$가 씌워져 결국 0의 패널티를 받는다.


<b> [ 두 번째 단어: 번역문에서 cover하지 못하는 경우 ] </b> <br>
- 입력 시퀀스의 두 번째 단어($x_2$)와 출력 시퀀스의 모든 단어에 대한 attention 확률($p_{2,j}$)을 계산한다.
- 번역문의 어떠한 위치에서도 attention weight가 높게 할당되지 못하여 $ min(\sum_{j=1}^{\lvert Y \rvert} p_{i,j}, 1.0)$가 매우 작아지고, $log$가 씌워져 결국 큰 음수의 패널티를 받는다.


이렇게 입력 시퀀스의 모든 단어에 대해 반복하게 되면, 두 번째 단어의 예시처럼 번역문에서 다루지 못한 단어가 존재할 경우 좋은 score를 얻기 힘들게 됩니다. 논문에서는 다양한 $\alpha$와 $\beta$의 조합에 대한 성능을 측정하였습니다. 영어를 프랑스어로 번역하는 task에서 각 조합에 따른 BLEU Score를 기록한 표를 아래에 첨부하겠습니다. 논문에서 소개하는 목적함수는 ML과 RL이 있는데, ML은 일반적으로 사용하는 Maximum Likelihood이고 RL은 뒷부분에서 설명하도록 하겠습니다(논문을 제 방식대로 재구성하다보니 순서가 꼬였습니다). 아래의 표는 ML을 목적함수로 하여 학습한 결과입니다. Length normalization과 coverage penalty를 적용했을 때 BLEU Score가 약간 향상하는 것을 확인할 수 있습니다. 참고로 또다른 목적함수인 RL까지 학습했을 때에는 큰 효과를 얻지 못하였다고 합니다.  결과적으로 논문의 실험에서는 $\alpha = 0.2, \beta = 0.2$ 로 설정하여 진행하였습니다.

<img src="/assets/figures/gnmt_ml.png" width="70%">



### Model Parallelism
병렬 처리에는 크게 Model Parallelism과 Data Parallelism 두 가지 유형이 있습니다. 대규모 데이터, 그리고 큰 모델을 학습시키기 위해 병렬화에 굉장히 신경을 많이 쓴 것을 논문에서 확인할 수 있었습니다. 두 유형의 간단한 특징에 대해 비교해보겠습니다. 병렬처리 방법론에 관련된 더 자세한 내용은 참고자료에 첨부해 두었습니다.


첫 번째로는 Model Parallelism입니다. 이 방법은 모델이 너무 커 하나의 GPU에 다 담을 수 없는 상황에서 모델을 나누어 여러 개의 GPU에 할당하는 방법입니다. 하나의 모델을 나누어 할당하는 방법이기 때문에 장비간의 통신이 많이 일어나는데, 통신에 대한 비용(communication cost) 역시 만만치 않기 때문에 모델 내 파라미터 간 통신이 최소화가 되도록 나누어 할당하는 것이 좋다고 합니다. 본 논문에서는 인코더와 디코더를 각각 나누어 할당했다고 언급합니다. 또한 인코더 내부에서는 앞서 설명한 것과 같이, 가장 첫 레이어만 bidirectional하게 구성하였기 때문에 나머지 레이어에 대해서는 병렬적인 연산을 할 수 있었습니다. 그 밖에도 디코딩 시 인코더의 가장 마지막 레이어와 디코더의 가장 첫 레이어만을 attention 연산에 활용하는 등의 방법이 존재합니다.


두 번째로는 Data Parallelism입니다. 우리는 대규모 병렬처리를 위해 GPU 장비를 사용하지만, 엄청나게 큰 데이터셋을 한 번에 담는 것은 불가능합니다. 따라서 미니배치 단위로 연산을 수행하게 되는데, 하나의 미니배치에 포함되는 데이터가 적으면 전체 데이터셋에 대한 gradient를 대표하지 못하기 때문에 수렴이 굉장히 늦어지게 됩니다. Data Parallelism은 각 GPU에서 미니배치에 대한 gradient를 계산한 후 이를 하나로 합치는 방법입니다. 일반적으로 우리가 아는 배치 단위의 병렬 학습 기법은 Data Parallelism이라 생각하시면 될 것 같습니다.


## 2. 데이터에 적용된 기술(Segmentation Approaches)

### Wordpiece

일반적으로 기계번역을 수행할 때에는 고정된 크기의 어휘 사전(vocabulary)을 만들어 두고 진행합니다. 그러나 실제 번역은 어휘 사전이 무한하다고 봐도 무방합니다. 사용자가 어떤 언어를 입력할 지 전혀 알지 못하기 때문입니다. 고정된 크기의 어휘 사전에 존재하지 않는 단어를 OOV(out-of-vocabulary)라 하는데, 기계번역에서는 이 OOV를 다루는 일이 굉장히 중요한 이슈입니다. 


WordPiece에 앞서 <b>Byte Pair Encoding(BPE)</b>에 대해 간단하게 짚고 넘어가겠습니다. BPE는 문장을 character 단위로 나누어 어휘 사전에 초기화한 후, 등장 빈도에 따라 character를 병합하여 어휘 사전에 추가하는 일을 일정 횟수(사용자 지정) 반복합니다. 그 결과로써 어휘 사전에 없는 단어가 입력으로 들어와도 기존의 character가 결합된 subword에 의해 OOV로 분류되지 않을 가능성이 높으며, 등장 빈도에 따른 subword 결합 방법이기 때문에 어느정도 문법적인 해석력도 갖춘 방법이라 할 수 있습니다.


WordPiece는 구글의 한국어 및 일본어 서비스를 시행하면서 두 언어의 infinite vocabulary 문제를 해결하기 위해 등장한 방법론입니다. BPE에서는 단어의 빈도에 따라 병합을 결정했다면, WordPiece에서는 두 단어의 병합이 학습 데이터(corpus)의 likelihood를 가장 높이는 경우 병합이 진행됩니다. 즉 언어 모델을 미리 학습을 해야 하는 것입니다. 병합 과정은 지정된 단어 개수를 초과하거나 likelihood가 일정량 이하로 증가할 때 종료됩니다. WordPiece를 사용하여 단어를 분절한 대표적인 예시는 아래와 같습니다.

```
[Word] Jet makers feud over seat width with big orders at stake
[WordPiece] _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake
```

WordPiece의 예시에서 나타나는 ```_``` 기호는 원 문장의 띄어쓰기와 동일한 의미를 갖지만, 단어의 분절과 구분하기 위해 사용되었습니다. 분절된 단어를 원 문장으로 쉽게 복원하기 위한 목적도 있습니다. 본 논문에서는 번역 업무에서 source/target 언어에 대해 동일한 WordPiece 모델을 사용하였습니다. 그렇게 되면 두 언어에서 동일하게 등장하는 문장은 정확하게 같은 방식으로 분절되기 때문에 모델이 토큰을 학습하기 쉬워진다고 합니다.

### Mixed Word/Character Model 

일반적으로 어휘 사전에 존재하지 않는 단어는 'Unknown'을 의미하는 ```<UNK>``` 토큰으로 표현됩니다. 그러나 본 논문에서는 모든 미등록단어를 하나의 토큰으로 '뭉개는' 방법 대신 아래와 같은 형태로 미등록단어를 처리합니다. 'Miki'라는 단어가 어휘 사전에 없다고 가정하겠습니다.

```
# 일반적인 경우
Miki → <UNK>

# Mixed Word/Character Model
Miki → <B>M <M>i <M>k <E>i
```

특수 토큰 ```<B>```는 Begin, ```<M>```은 Middle, 그리고 ```<E>```는 End를 의미합니다. 이렇게 단어 내 각 글자의 상대적 위치를 명시하고 어휘 사전 내의 단어를 포함하는 방법을 적용하면 디코딩 시에도 ```<B>```, ```<M>```, ```<E>``` 토큰이 포함되어 생성됩니다. 그러나 후처리를 통해 원래 단어로 복구하는 일은 (특수 토큰만 제거하면 되기 때문에)어렵지 않습니다.


## 3. 평가 방법에 적용된 기술

### Mixed Objective Function

기계번역의 최종 목표는 사람이 직접 해석한 것과 동일한 정도의 번역문을 출력하는 것입니다. 일반적으로 기계번역에서는 Maximum Likelihood Objective를 목표로 학습을 진행합니다. Maximum Likelihood Objective를 표현하면 아래와 같습니다.

$${ \mathcal{O}_{ML}(\theta) = \sum_{i=1}^N log P_\theta (Y^{\ast (i)} \mid X^{(i)}) }$$


 그러나 논문에서는 Maximum Likelihood가 [BLUE Score](https://donghwa-kim.github.io/BLEU.html)로 측정되는 Reward Function을 제대로 반영하지 않음을 지적합니다. 즉 학습의 예측 번역문과 정답 번역문 사이에 괴리가 있을 수 있다는 말입니다.


[이 논문](https://www.aclweb.org/anthology/E06-1032.pdf)에서는 BLUE Score가 높을 경우 번역의 질이 높아질 수 있지만, 좋은 번역일수록 BLUE Score가 높다는 명제는 성립하지 않음을 증명합니다. 정답 번역문이라 해도 다양한 형태가 존재할 수 있기 때문에, BLEU Score는 여러 개의 정답문으로 어느 정도의 variation을 만드는 시도를 합니다. 어느 정도의 variation을 보존하기 위하여 [WER(Word Error Rate)](https://en.wikipedia.org/wiki/Word_error_rate)의 String Edit Distance보다는 덜 엄격한 <i>modified n-gram precision</i> 기법을 적용합니다.


그러나 제기하는 문제는 BLUE Score에 가해지는 명시적인 제약(n-gram 매칭이 여러 정답 번역문으로부터 어떻게 추출되는지 등)이 너무 적어서 너무 많은 variation이 발생한다는 것입니다. 이렇게 의미적으로나 문법적으로 엄격한 제약이 존재하지 않는 경우, 동일한 BLUE Score로 측정되는 경우의 수가 굉장히 많아져 아래와 같은 상황이 발생할 수 있습니다.

- 다른 예측 번역문과 같은 BLUE Score를 갖지만 사람에 의한 평가는 더 나쁜 예측 번역문이 존재
- 번역의 질을 높이지 않으면서 BLUE Score만을 높일 수 있음


더 자세한 내용에 대해서는 해당 논문을 참고하시고, 결론적으로는 reward를 반영하기 위해 Maximum Likelihood와 별개로 강화학습에 기반한 또다른 objective를 제안합니다. 식은 아래와 같습니다.


$${ \mathcal{O}_{RL}(\theta) = \sum_{i=1}^N \sum_{Y\in \mathcal{Y}} P_\theta (Y \mid X^{(i)}) \; r(Y, Y^{\ast (i)}) }$$


이 목적함수는 MLE로 학습된 모델에 추가적으로 task에 대한 보상을 직접적으로 최적화하는 것이 목적입니다. 여기서 $ r(Y, Y^{\ast (i)}) $는 per-sentence score로, 모든 출력 문장 $Y$에 대해 (특정 길이까지의) 기댓값을 계산한 수치라 합니다. 최종적으로는 아래와 같은 목적함수를 제안합니다.

$${ \mathcal{O}_{Mixed}(\theta) = \alpha \ast \mathcal{O}_{ML}(\theta) + \mathcal{O}_{RL}(\theta) }$$

그리고 아래와 같은 flow로 학습을 진행합니다.

1. Maximum Likelihood로 수렴할 때까지 학습 진행
2. (Optional) 수렴 이후 Mixed Objective로 최적화

### GLEU Score

앞서 언급했듯이 본 모델에서는 강화학습이 포함됩니다. 따라서 강화학습 실험시에는 BLEU Score 대신 GLEU Score를 제안하여 사용합니다. GLEU Score는 예측값과 실제값의 n-gram에 대한 recall과 precision을 계산하여 그 중 가장 작은 수치를 나타냅니다. 논문에서는 BLEU Score가 corpus 단위로 디자인되었기 때문에 sentence 단위에서 사용될 때에는 몇 가지 부적절한 특징이 있어 GLEU를 사용하였다고 하는데, [BLEU 원 논문](https://www.aclweb.org/anthology/P02-1040.pdf)에서는 아래와 같이 언급하여 살짝 의아했습니다.

```
Although one typically evaluates MT systems on a corpus of entire documents, our basic unit of evaluation is the sentence.
```

어쨌든 실험적으로는 GLEU로 강화학습 실험을 진행했을 때, BLEU와 마찬가지로 corpus 단위에서도 잘 동작하면서 sentence 단위의 reward objective 역시 잘 측정하였다고 합니다.



## 4. 경량화에 적용된 기술

### Quantization
기계번역 모델을 실제 서비스로 배포할 때 발생하는 주된 어려움 중 하나는 추론(inference) 시 연산 부담이 발생한다는 것입니다. 이를 줄이기 위해 'Quantized Inference'라는 방법이 사용됩니다. Quantization 기술 자체는 본 논문에서 처음 공개하는 기술은 아니고, 부동 소수점의 표현 범위를 줄여 같은 연산 장비에서 더 효율적인 연산을 가능하게 하는 방법론입니다. 대신 모델 성능과의 약간의 trade-off가 존재할 수 있습니다.


### Quantization for Deep LSTM
하지만 대부분의 quantization 연구는 얕은 층으로 구성된 CNN 기반 모델을 중심으로 연구되어 왔습니다. 따라서 깊은 층으로 구성된 LSTM 신경망에서는 기존 quantization이 오히려 악영향을 끼쳤다고 합니다. 따라서 새로운 접근을 제안했는데요, 학습 중 약간의 제약을 추가하여 번역의 질을 낮추지 않으면서도 효과적으로 quantization을 수행할 수 있다고 합니다. 간단히 말씀드리자면 LSTM 내 특정 변수값과 logit의 범위에 각각 제한(bound)을 두어 최종적으로는 모델 학습 시 수렴을 방해하지도 않고 디코딩 시 결과물의 질이 나빠지지도 않게 됩니다.


아래의 그림은 quantization 적용 여부에 따른 log perplexity 차이입니다. Quantization을 했는데 오히려 성능이 더 좋게 나오는 것을 확인할 수 있습니다. 이는 변수값의 bound가 regularization 역할을 수행했기 때문이라 주장합니다.

<img src="/assets/figures/gnmt_quant.png" width="70%">


### Tensor Processing Unit(TPU)
Quantization의 효과를 확인하기 위해 논문에서는 영어를 프랑스어로 번역하는 task의 inference 속도와 품질을 비교하였습니다. 또한 장비에 따른 비교도 하였는데요, CPU와 GPU, 그리고 구글의 자체 장비인 TPU를 사용하였습니다. 학습은 ML 목적함수로만 하였고, quantization을 적용하였습니다. 단, 디코딩 시 CPU와 GPU에서는 quantization을 적용하지 않은 full-precision으로 진행하였고, TPU로 디코딩하는 과정에서만 quantization이 일부 적용되었습니다.


결과는 아래와 같습니다. Quantization을 진행하였음에도 불구하고 성능의 하락이 거의 없는 것을 확인할 수 있습니다. 그리고 quantization과 TPU를 모두 적용했을 때의 엄청난 디코딩 속도를 체감할 수 있습니다. 참고로 이후의 실험 부분에서는 evaluation 단계에서는 항상 CPU로 디코딩을 진행하고, 실제 서비스 시에만 TPU를 사용하였다고 언급합니다.

<img src="/assets/figures/gnmt_tpu.png" width="70%">



# [3] 실험
실험에 사용한 데이터는 아래와 같습니다.

- Training 데이터 1: WMT'14 En→Fr (영어 → 프랑스어 벤치마크 데이터셋, 36M 문장)
- Training 데이터 2: WNT'14 En→Ge (영어 → 독일어 벤치마크 데이터셋, 5M 문장)
- Validation 데이터: newstest2012 + newstest2013
- Test 데이터 1: newstest2014
- Test 데이터 2: 실제 서비스에서 활용가능한지를 확인하기 위한 구글 내부 데이터
    - 영어 → 프랑스어
    - 영어 → 스페인어
    - 영어 → 중국어


성능에 대한 평가는 BLEU Score와 사람의 평가로 진행됩니다. 또한 처음에 Adam optimizer로 60k까지 학습 후 SGD로 변경하여 학습합니다. Adam 하나로만 학습하면 잘못된 지점으로 최적화될 가능성이 높다는 이유였습니다. 실험 환경에 대한 더 자세한 설정은 생략하고 결과를 살펴보겠습니다.


## 1. 두 개의 optimizer를 사용한 효과

Optimizer 조합에 따른 성능 차이는 아래에서 확인할 수 있습니다. 빨간색 그래프가 제안하는 방법론인데요, 관측할 수 있는 현상들은 다음과 같습니다.

- Step이 지날 수록 제안하는 방법의 성능이 좋아짐 
- SGD보다 Adam이 초반에 빠르게 수렴함
- 제안하는 방법에서 초반에 log perplexity가 잠깐 솟아오르는데, 이는 Adam에서 SGD로 전환되는 지점임

<img src="/assets/figures/gnmt_opt.png" width="70%">


## 2. Objective Function과 Segmentation에 따른 비교

모델 학습 시에는 ML 목적함수로 완전히 수렴할 때까지 학습한 후, RL 목적함수로 fine-tuning을 진행합니다. 그리고 아래와 같은 조합에 대해 비교를 시행하였습니다.

- Word-based vocabulary
- Character-based vocabulary
- Mixed word/character-based vocabulary
- Wordpiece-based vocabulary

### Maximum Likelihood로 학습

위 조합들에 대해 ML로 학습한 후 결과를 확인하면 아래와 같습니다. 성능 비교를 위해 이전 방법론들의 수치까지 함께 사용하였습니다.

<img src="/assets/figures/gnmt_enfr.png" width="70%">

영어를 프랑스어로 번역하는 task에서는 32k개의 vocabulary 크기를 갖는 wordpiece 모델이 가장 높은 속도를 보입니다. 디코딩 속도 역시 굉장히 빠릅니다. Character-based의 경우 의외로 높은 BLEU Score를 보이지만, 디코딩 속도가 굉장히 느리고 문장 길이가 늘어날수록 학습 속도도 굉장히 느려진다고 합니다. 


<img src="/assets/figures/gnmt_enge.png" width="70%">

아래의 영어를 독일어로 번역하는 task는 일반적으로 더 어려운 task에 속합니다. 데이터 수도 적고 형태학적으로 독일어가 굉장히 풍부하기 때문에 vocabulary의 크기가 커야 하기 때문입니다. 하지만 이 task에서도 wordpiece 기반 또는 mixed word/character의 모델이 좋은 성능을 보입니다.


### RL-refined로 학습
이번에는 ML로 학습한 모델에 추가적으로 RL을 fine-tuning한 모델에 대한 성능 비교입니다. 결과는 아래와 같습니다. 영어를 프랑스어로 번역하는 task에서는 성능 향상이 있었지만, 독일어로 번역하는 상황에서는 약간 감소하였습니다. 저는 이 결과를 보고 서비스에 도입하는 것이 좋은지에 대해서는 사실 감이 잘 오지 않습니다. 독일어만큼이나 형태학적으로 풍부한 단어들이 더 있을테고, 향상된 정도가 production level에서는 정성적으로 얼마나 차이가 나는지에 대해 확인할 방법이 없었기 때문입니다. 그러나 저자들은 coverage penalty가 RL에서는 큰 효과가 없었다는 실험(표는 이 글에 수록되지 않음)을 바탕으로 length normalization, coverage penalty 같은 기법을 사용하지 않고 beam search를 적용하면 더 좋은 결과를 얻을 것이라 말합니다.

<img src="/assets/figures/gnmt_rl.png" width="70%">


또한 더 좋은 성능 수치를 얻기 위해 논문에서는 8개의 모델에 대한 앙상블을 적용합니다. BLEU Score를 측정하였고, 사람이 평가한 Score까지 측정하였습니다.

<img src="/assets/figures/gnmt_t7.png" width="70%">

위 표에 따르면 프랑스어로 번역할 때 이전의 모델에 비해 RL까지 적용한 모델의 성능이 가장 높은 BLEU Score를 갖습니다.

<img src="/assets/figures/gnmt_t8.png" width="70%">

역시 위의 표에 따르면 독일어로 번역할 때 이전의 모델에 비해 RL까지 적용했을 때 모델의 성능이 약간 더 높은 BLEU Score를 갖습니다. 위에 첨부한 Table 5에서 단일 모델(WPM-32K)의 BLEU Score가 24.61인 것을 보면 앙상블의 효과는 확실해 보입니다.


<img src="/assets/figures/gnmt_t9.png" width="70%">

위의 표는 Phrase-based 기계번역 모델(PBMT)과 제안하는 방법론들 간의 비교입니다. BLEU Score와 사람에 의한 평가 모두 PBMT보다 좋지만, 사람에 의한 평가 측면에서 RL의 효과는 거의 없는 것 같습니다. 논문에서는 그 이유를 아래와 같이 설명합니다. 성능이 기대한 것과 다를 때 그 이유를 자세히 분석한 것 같아 인상깊었습니다.

- 사람이 평가할 때 500개의 샘플밖에 사용하지 않았음
- RL 적용으로 인한 BLEU Score가 앙상블한 것 치고는 상대적으로 덜 향상하여 사람이 보기에는 큰 차이가 없음
- BLEU Score와 실제 평가 간의 불일치

어떻게 보면 핑계(?) 같지만서도, 아래의 표를 제시하며 수행하는 task가 얼마나 모호한지를 어필합니다. 500개의 샘플에 대해서 PBMT, GNMT, 그리고 사람의 번역을 비교하였는데, 사람이 번역한 것이 더 낮게 평가되는 경우도 꽤 있다는 것을 강조하며 그래도 PBMT보다는 제안하는 모델(GNMT)이 훨씬 좋다고 주장합니다. 참고로 표에 제시된 0~6의 Score는 아래와 같은 의미를 갖습니다. 일부에 대해서만 적어둡니다.

- 0: 완전히 말도 안 되는 번역
- 2: 원문의 의미 약간을 갖고 있지만 중요한 부분들이 빠짐
- 4: 원문의 의미 대부분을 갖고 있지만 약간의 문법 오류가 있음
- 6: 완벽한 번역, 번역의 의미가 원문과 완전히 일치하고 문법적으로도 맞음

<img src="/assets/figures/gnmt_cnt.png" width="70%">


## 3. 실제 데이터에 적용했을 때의 효과
이번에는 구글 내부 데이터를 사용하여 성능을 평가해보았습니다. 놀랍게도 여기서는 RL을 추가로 학습하는 것이 실제 번역의 질을 높이는 데 큰 효과가 없다는 것을 쿨하게 인정하고 사용하지 않습니다(Technical Report라 조금 더 자유롭게 표현한 것인가 싶었습니다). 비교군은 총 세 가지로 설정하였습니다.

- 기존 구글에서 사용하던 phrase-based 통계적 번역 시스템
- 본 논문에서 제안하는 GNMT
- 기존 언어와 번역 언어 모두 유창한 사람의 번역

결과는 아래와 같습니다. 전반적으로 기존 서비스보다 더 좋은 성능을 보이는 것이 요지입니다.

<img src="/assets/figures/gnmt_prod.png" width="70%">



# [4] 마치며

실제 서비스에 이용하는 모델에 대한 설명이라 그런지 굉장히 실용적인 노하우가 많이 담겨있는 논문이었습니다. 하나의 모델에 여러 개의 굵직한 연구 분야에 대한 방법론이 적용되어 다양한 방법론에 대해 가볍게라도 이해할 수 있었고, 논문에서만 제안했던 방법론이 실제로는 어떠한 이슈가 있고 이를 해결하기 위해 어떤 트릭을 사용했는지 설명되어 있어서 여러모로 유익한 논문이라 생각합니다. 거의 논문 전체가 실험 설계...라고 보아도 될 것 같습니다.


연구실 세미나의 일환으로, 해당 논문에 대한 [리뷰 영상](https://www.youtube.com/watch?v=XMOtdAP_AHY&list=PLetSlH8YjIfUpPbSAfsY4zBJfztlH9CSQ&index=1)을 제작하였습니다. 필요하신 분은 참고해주시면 감사하겠습니다. 


# [5] 참고자료 
- [[Paper] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)
- [[Paper] Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [[Paper] 딥러닝 분산 학습 관련 연구](https://lyusungwon.github.io/assets/publications/DistributedDeepLearningTrainingOverview.pdf)
- [[Paper] Japanese and Korean Voice Search](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37842.pdf)
- [[Paper] BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf)
- [[Paper] Re-evaluating the Role of BLEU in Machine Translation Research](https://www.aclweb.org/anthology/E06-1032.pdf)
- [[Paper] 경량 딥러닝 기술 동향](https://ettrends.etri.re.kr/ettrends/176/0905176005/34-2_40-50.pdf)
- [[Book] Natural Language Understanding with Distributed Representation](https://arxiv.org/abs/1511.07916)
- [[Post] GitBook: Google Neural Machine Translation](https://kh-kim.gitbooks.io/pytorch-natural-language-understanding/content/productization/gnmt.html)
- [[Post] Weekly NLP: NLP의 궁예 등장? 관심법으로 번역을 잘해보자](https://jiho-ml.com/weekly-nlp-23/)
- [[Post] Data Parallelism VS Model Parallelism in Distributed Deep Learning Training](https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/)
- [[Post] Word Piece Model](https://lovit.github.io/nlp/2018/04/02/wpm/)
- [[Post] BLEU Score](https://donghwa-kim.github.io/BLEU.html)
- [[Wikidocs] 바이트 페어 인코딩](https://wikidocs.net/22592)
- [[Wikipedia] Word error rate](https://en.wikipedia.org/wiki/Word_error_rate)
- [[Github] sentencepiece](https://github.com/google/sentencepiece)