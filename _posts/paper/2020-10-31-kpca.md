---
layout: post
title: "Face Recognition Using Kernel Eigenfaces"
description: "Ming-Hsuan Yang, Narendra Ahuja, David Kriegman (ICIP 2000)"
date: 2020-10-31
categories: paper
tags: [review, code, machinelearning, kernel method, pca]
image: 
---


# [1] Backgrounds

논문이 나올 당시에는 PCA나 Eigenface 기법이 얼굴 인식, 추적 등에 많이 사용되었습니다. 이 두 가지 개념에 대해 간단히 짚고 넘어가도록 하겠습니다.

## Principle Component Analysis (PCA)
데이터의 차원이 많을 때 발생하는 '차원의 저주' 문제를 해결하기 위해서는 핵심적인 특징만을 사용할 필요가 있습니다.
크게 변수선택법과 변수추출법을 사용할 수 있는데, PCA(주성분분석)는 데이터를 나타내는 새로운 특징을 추출하는 변수추출법 중 하나입니다.


PCA의 목적은 원 데이터의 분산을 최대한 보존하는 기저(basis)를 찾는 것입니다. PCA를 수행하기 위해서는 아래와 같은 과정을 거칩니다.


### 1. 데이터의 평균을 0으로 조정 
목적함수를 최적화할 때 편의를 위한 조정입니다. 물론 나중에 평균을 조정하는 방법도 있습니다.


### 2. 목적함수 최적화 
먼저 임의의 기저 ${ w}$를 정의하고, 데이터를 기저에 사영(project)시킨 후 분산을 최대화하는 기저를 찾습니다. 사영 후의 공분산행렬(${ V}$)을 식으로 나타내면 아래와 같고, ${S }$는 표본공분산을 의미합니다.

$${ V={1\over n}(w^TX-m)(w^TX-m)^T = {1 \over  n}w^TXX^Tw=w^TSw }$$

그리고 아래와 같은 제약 조건 하에서 ${ V}$를 최대화해야 합니다. 기저의 크기를 1로 normalize하기 위한 설정입니다.

$${ max \; w^TSw^T \;\;\;\;\;\; s.t. w^Tw=1 }$$

제약식이 있는 목적함수를 최적화하기 위해서는 라그랑주 승수법을 이용하여 보조 목적 함수를 만듭니다. 
그리고 이를 최적화하기 위해 미분값이 0이 되는 지점을 찾으면 행렬 ${S }$에 대한 고윳값과 고유벡터를 구할 수 있습니다.

### 3. 분산을 최대로 보존하는 벡터와 보존량 확인
앞선 과정을 통해 얻은 고유벡터는 원 데이터를 사영하는 기저가 될 것이고, 각 고유벡터에 대응하는 고윳값은 사영 시 보존되는 분산의 양을 의미합니다. 따라서 각 기저에 대한 분산량을 비교하여 분산을 가장 많이 보존하는 특징들을 추출할 수 있게 됩니다.

## Eigenface
Eigenface는 PCA를 얼굴 이미지에 적용한 개념입니다. 이미지는 ```(가로) X (세로)```의 형태로 구성되어 있는데, 이를 일렬로 벡터화하면 ```(가로 X 세로)``` 형태가 됩니다. 이 벡터에 PCA를 적용하면 동일한 차원의 주성분 벡터(고유벡터)를 추출할 수 있습니다. 주성분 벡터는 기존 데이터와 동일한 차원이기 때문에 이를 이미지의 shape으로 변형하여 eigenface를 만들 수 있습니다. Eigenface를 추출하면 아래와 같은 그림으로 나타납니다. 고윳값이 클수록 얼굴의 굵직한 특징이 잘 나타나고, 작아질수록 세부적인 특징이 나타납니다. 따라서 Eigenface를 많이 추출할수록 원본 얼굴과 가깝게, 적게 추출할수록 일반적인 얼굴의 특징만 남게 됩니다.

<img src="/assets/figures/kpca_ef.png" width="70%">



# [2] Kernel PCA
본 논문에서 사용한 방법론인 Kernel PCA는 아래와 같은 과정으로 진행됩니다.

## 1. 고윳값과 고유벡터를 이용한 식 전개
본 논문에서 사용하는 Kernel PCA(KPCA)는 PCA의 일반화된 형태입니다. 현실세계의 데이터는 저차원 선형 공간에서 분류가 제대로 되지 않는 경우가 굉장히 많습니다. 따라서 데이터를 고차원에 mapping한 후 선형 분류를 시도하는 방법론인 Kernel 방법론은 SVM 등에도 적용되어 복잡한 패턴을 나타내는 데 유용하게 쓰였습니다. 


Kernel PCA는 기존 PCA의 방법론에서 데이터를 고차원으로 mapping하는 함수(${ \Phi }$)가 추가된 형태입니다. 역시 mapping 이후 데이터의 평균을 0이라 가정하면 아래와 같은 식으로 표현할 수 있습니다.

$${ m^\Phi = {1 \over N} \sum_{i=1}^N {\Phi(x_i)}=0 }$$

이 가정 하에서 mapping된 데이터의 공분산행렬(${ C^\Phi}$)은 아래와 같이 구할 수 있습니다.

$${ C^\Phi = {1\over N} \sum_{i=1}^N (\Phi(x_i)-m^\Phi)(\Phi(x_i)-m^\Phi)^T = {1 \over N}\sum_{i=1}^N \Phi(x_i)\Phi(x_i)^T   \; \cdots \cdots \cdots \; ①  }$$

또한 고윳값(${\lambda }$)과 고유벡터(${v }$)의 정의에 따라 아래와 같은 식을 얻을 수 있습니다.

$${ C^\Phi v_k =\lambda_k v_k  \; \cdots \cdots \cdots \; ②}$$

그리고 주성분벡터는 [입력 데이터의 선형 결합으로 표현될 수 있기](https://www.quora.com/Are-the-eigenvectors-of-a-matrix-A-linear-combinations-of-the-columns-of-A) 때문에 아래와 같은 식을 얻을 수 있습니다.

$${ v_k = {1 \over N} \sum_{i=1}^N \alpha_{ki} \Phi(x_i) \; \cdots \cdots \cdots \; ③}$$

따라서 식 ①, ②, 그리고 ③을 조합하면 아래와 같이 전개됩니다.

$${ C^\Phi v_k =\lambda_k v_k }$$

$${ {1 \over N}\sum_{i=1}^N \Phi(x_i)\Phi(x_i)^T v_k =\lambda_k v_k }$$


$${ {1 \over N}\sum_{i=1}^N \Phi(x_i)\Phi(x_i)^T \sum_{i=1}^N \alpha_{ki} \Phi(x_i) =\lambda_k \sum_{i=1}^N \alpha_{ki} \Phi(x_i) \; \cdots \cdots \cdots \; ④ }$$


## 2. Kernel Trick을 이용한 정리
잠깐 kernel trick에 대해 짚고 넘어가겠습니다. 우리가 데이터를 고차원에 mapping하는 이유는 데이터를 더 잘 분리하는 hyperplane을 찾기 위함입니다. 그래서 최적화하려는 식을 보면 고차원에 mapping된 데이터의 내적 형태로 표현되어 있습니다. 그런데 원하는 바가 있어 고차원 데이터를 사용한다고 해도 이를 mapping하는 함수를 찾기 위해서는 많은 cost가 소모됩니다. 


Kernel trick은 최적화 식에서 고차원 데이터의 내적을 **원본 공간 데이터의 함수(${ K}$)** 를 통해 효율적으로 표현하는 방법입니다. 이 ${K }$는 [Mercer's Condition](https://en.wikipedia.org/wiki/Mercer%27s_theorem)을 만족하는 임의의 함수입니다. 간단하게 언급하자면 Kernel Matrix는 대칭행렬이어야 하고, positive semi-definite 행렬이어야 합니다. 대표적으로는 polynomial, gaussian(rbf), sigmoid 커널 함수가 있습니다. 이를 식으로 나타내면 아래와 같습니다. Kernel trick과 관련하여 더 자세한 설명은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=RKMiTJAnLy8&list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&index=11)를 참고하시면 될 것 같습니다.

- Polynomial: ${ K(x,y)=(x\cdot y+c)^d, \;\; c>0 }$
- Gaussian(RBF): ${ K(x,y)=exp({- { {\lVert x-y \rVert}^2  } \over 2\sigma^2  }), \;\; \sigma \neq 0 }$
- Sigmoid: ${ K(x,y) = tanh(a(x\cdot y)+b), \;\; a,b \ge 0 }$


본론으로 돌아와서, 식 ④의 양 변에 새로운 mapping 함수(${ \Phi(x_l)}$)를 곱하여 아래와 같은 식을 유도할 수 있습니다. 유도 과정에서 kernel trick을 적용한 후 matrix notation으로 변환하는 과정이 포함되어 있습니다.

$${ {1 \over N}\sum_{i=1}^N \Phi(x_l)\Phi(x_i) \sum_{i=1}^N \alpha_{ki} \Phi(x_i)^T \Phi(x_i) =\lambda_k \sum_{i=1}^N \alpha_{ki} \Phi(x_l)^T\Phi(x_i) }$$


$${ {1 \over N}\sum_{i=1}^N K(x_l,x_i) \sum_{i=1}^N \alpha_{ki} K(x_i,x_j) =\lambda_k \sum_{i=1}^N \alpha_{ki} K(x_l,x_j) }$$


$${ K^2 \alpha_k = \lambda_k NK\alpha_k }$$


$${ K\alpha_k = \lambda_k N\alpha_k }$$

따라서 ${\alpha_k }$는 ${ K }$의 고유벡터가 됩니다.

## 3. ${ k}$번째 기저에 ${x }$를 사영한 값 구하기
이제 고차원으로 mapping한 데이터를 기저에 사영한 결과(${ y}$)를 확인할 수 있습니다. 식으로 나타내면 아래와 같습니다.

$${ y_k(x) = \Phi(x_i)^Tv_k }$$

여기서 식 ③을 대입하고 kernel trick을 적용하면 다음과 같이 표현됩니다.

$${ y_k(x) = \Phi(x_i)^Tv_k = \Phi(x_i)^T {1 \over N} \sum_{i=1}^N \alpha_{ki} \Phi(x_i) = \sum_{i=1}^N \alpha_{ki} K(x,x_i) }$$

정리하자면 고차원 feature space로의 mapping을 하지 않고도 원래 공간에서의 함수로 이를 표현할 수 있다는 말입니다. 




# [3] 코드
오래 전의 논문이라 그런 것인지는 몰라도, 실험에 대한 명세가 충분하지 않아서 같은 task에 대해 임의로 실험을 진행하였습니다. 실험 내용은 얼굴 이미지에서 추출한 특징을 사용하여 사람을 구분하는 Face Recognition Task 입니다. 실험 환경은 아래와 같습니다. 기본적으로 Scikit-learn 패키지를 이용하여 진행하였습니다.


- 데이터셋: 논문에서 사용한 Olivetti 데이터셋
- 분류기: KNN Classifier (K=5)
- PCA 모델: 논문에서 사용한 모델에 rbf 커널 추가
    1. PCA(Eigenface)
    2. Kernel PCA, kernel=poly, degree=2
    3. Kernel PCA, kernel=poly, degree=3
    4. Kernel PCA, kernel=poly, degree=4
    5. Kernel PCA, kernel=poly, degree=10
    6. Kernel PCA, kernel=rbf
- 주성분 개수: 논문의 설정대로 40, 50, 60, 80개
- 평가지표: Accuracy Score
- 랜덤시드: 7, 11, 711로 3회 반복 후 결과 평균

## 1. 라이브러리 불러오기 및 데이터셋 확인
Olivetti 데이터셋은 scikit-learn 라이브러리에서 제공합니다. 총 400장의 64x64 얼굴 이미지이며, 총 40명의 얼굴이 다양한 조건에서 10장씩 포함되어 있습니다. 논문에서는 연산량을 줄이기 위해 downsample을 했지만 시간이 많이 흘러 컴퓨팅 파워가 좋아졌기 때문에 별도로 수행하지 않았습니다.

```python
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_olivetti_faces
from sklearn.decomposition import PCA
from sklearn.decomposition import KernelPCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

faces = fetch_olivetti_faces() # load dataset

def face_plot(faces, target):
    assert target < 40, "target should be smaller than 40"
    
    tgt_faces = faces.images[faces.target==target]
    
    fig = plt.figure(figsize=(10, 5))
    plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)
    for i in range(2):
        for j in range(5):
            k = i * 5 + j
            ax = fig.add_subplot(2, 5, k+1)
            ax.imshow(tgt_faces[k], cmap=plt.cm.bone)
            ax.grid(False)
            ax.xaxis.set_ticks([])
            ax.yaxis.set_ticks([])
    plt.suptitle(f"Faces of the figure index {target}")
    plt.tight_layout()
    plt.show()

face_plot(faces, 7) # 7번 인덱스에 해당하는 사람의 얼굴 확인
```

<img src="/assets/figures/kpca_faceplot.png" width="70%">


## 2. 데이터 분리
```python
# train : test = 0.7 : 0.3
X = faces.data
y = faces.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)

print(len(X_train), len(X_test), len(y_train), len(y_test)) # 280, 120, 280, 120
```


## 3. PCA 모델 적용
```python
n_comp = 40 # Option: [40, 50, 60, 80]
pca = PCA(n_components=n_comp).fit(X_train)
kpca_poly_2 = KernelPCA(n_components=n_comp, kernel='poly', degree=2).fit(X_train)
kpca_poly_3 = KernelPCA(n_components=n_comp, kernel='poly', degree=3).fit(X_train)
kpca_poly_4 = KernelPCA(n_components=n_comp, kernel='poly', degree=4).fit(X_train)
kpca_poly_10 = KernelPCA(n_components=n_comp, kernel='poly', degree=10).fit(X_train)
kpca_rbf = KernelPCA(n_components=n_comp, kernel='rbf').fit(X_train)

X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

X_train_kpca_poly_2 = kpca_poly_2.transform(X_train)
X_test_kpca_poly_2 = kpca_poly_2.transform(X_test)

X_train_kpca_poly_3 = kpca_poly_3.transform(X_train)
X_test_kpca_poly_3 = kpca_poly_3.transform(X_test)

X_train_kpca_poly_4 = kpca_poly_4.transform(X_train)
X_test_kpca_poly_4 = kpca_poly_4.transform(X_test)

X_train_kpca_poly_10 = kpca_poly_10.transform(X_train)
X_test_kpca_poly_10 = kpca_poly_10.transform(X_test)

X_train_kpca_rbf = kpca_rbf.transform(X_train)
X_test_kpca_rbf = kpca_rbf.transform(X_test)

```

### 3-1. Eigenface 확인
```python
# check eigenface
eigen_pca = PCA(n_components=n_comp).fit(X_train)
eigenfaces = eigen_pca.components_.reshape((n_comp, 64, 64))
eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]

def eigenface_plot(images, titles, n_row, n_col, h=64, w=64):
    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))
    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)
    for i in range(n_row * n_col):
        plt.subplot(n_row, n_col, i + 1)
        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)
        plt.title(titles[i], size=12)
        plt.xticks(())
        plt.yticks(())
        
eigenface_plot(eigenfaces, eigenface_titles, n_row=4, n_col=5) # 20개만 확인
```
<img src="/assets/figures/kpca_eigenplot.png" width="70%">


## 4. KNN Classifier로 분류 후 예측
```python
knn = KNeighborsClassifier(n_neighbors=5)

fit_list_X = [
    ('PCA', X_train_pca, X_test_pca),
    ('KPCA poly 2', X_train_kpca_poly_2, X_test_kpca_poly_2),
    ('KPCA poly 3', X_train_kpca_poly_3, X_test_kpca_poly_3),
    ('KPCA poly 4', X_train_kpca_poly_4, X_test_kpca_poly_4),
    ('KPCA poly 10', X_train_kpca_poly_10, X_test_kpca_poly_10),
    ('KPCA rbf', X_train_kpca_rbf, X_test_kpca_rbf),
]

for model, train_X, test_X in fit_list_X:
    print(f'\nMODEL: {model}')
    knn.fit(train_X, y_train)
    y_pred = knn.predict(test_X)
    print(f'** Accuracy: {accuracy_score(y_test, y_pred)}')

```


## 5. 결과 확인

<img src="/assets/figures/kpca_result.png" width="70%">

총 세 개의 랜덤시드를 적용하여 반복 실험 후 평균을 계산하였습니다. 상황에 따라 일반 PCA가 좋은 결과를 내는 경우도 있었습니다. '좋은 모델을 찾는 것'을 목표로 한다면 더 많은 주성분 개수 탐색, train/test 데이터 비율 조정, KNN 하이퍼파라미터 조정 등의 과정을 거치는 것이 맞지만, 논문의 조건 하에서 PCA와 KPCA의 성능을 비교하는 것이 목적으로만 실험을 진행한 점 참고해주시면 감사하겠습니다. Accuracy Score 뿐 아니라 Recall이나 Precision까지 확인하고자 할 땐 [scikit-learn의 classification_report 함수](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)를 사용하시면 됩니다.


# [4] 참고자료

- [[Paper] Face Recognition using Kernel Eigenfaces](https://ieeexplore.ieee.org/abstract/document/900886?casa_token=2KUeb1-yxrEAAAAA:FnOVB9HEuN6XNkmxJc-GOnTuo0LHM5x5ibd1saPRvgYvUwNuQjBX3KC1liYFgcUOYgWQwU5R3w)
- [[Lecture] Dimensionality Reduction: PCA](https://www.youtube.com/watch?v=bEX6WPMiLvo&list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&index=5)
- [[Lecture] Kernel-based Learning: KPCA](https://www.youtube.com/watch?v=6Et6S03Me4o&list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&index=14)
- [[Lecture] Kernel-based Learning: SVM(Soft Margin)](https://www.youtube.com/watch?v=RKMiTJAnLy8&list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&index=11)
- [[Wikipedia] Eigenface](https://en.wikipedia.org/wiki/Eigenface)
- [[Wikipedia] Mercer's Theorem](https://en.wikipedia.org/wiki/Mercer%27s_theorem)
- [[Post] 주성분분석(PCA)의 이해와 활용](https://darkpgmr.tistory.com/110)
- [[Quora] Are the eigenvectors of a matrix A linear combinations of the columns of A? ](https://www.quora.com/Are-the-eigenvectors-of-a-matrix-A-linear-combinations-of-the-columns-of-A)
