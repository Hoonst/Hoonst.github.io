---
layout: post
title: "Unsupervised Deep Embedding for Clustering Analysis"
description: "Junyuan Xie, Ross Girshick, Ali Farhadi (ICML 2016)"
date: 2020-10-09
categories: paper
tags: [review, code, deeplearning, unsupervised learning, clustering]
image: 
---

# [1] 아이디어 제안 배경

## Clustering의 연구 방향
클러스터링(Clustering)은 데이터 간 거리를 기반으로 그룹을 나누는 비지도학습 방법론입니다. 
일반적으로 데이터를 파라미터 공간에 mapping한 후, 데이터 간의 거리를 측정하여 적당한 그룹을 할당하는 식으로 진행됩니다.
클러스터링은 주로 아래와 같은 방향으로 많이 연구된다고 합니다.

- Feature Selection: 군집을 결정하는 요인이 무엇인가?
- Distance Function: 최적의 거리 측정법은 무엇인가?
- Grouping Method: 어떻게 하면 군집을 효과적으로 생성할 수 있을까?
- Cluster Validation: 생성된 군집에 대한 검증은 어떻게 할까?


## 기존 Clustering 방법의 한계

대표적인 클러스터링 기법으로는 K-Means 클러스터링과 Gaussian Mixture Model(GMM)이 있습니다.
이 기법들은 빠른 속도와 확장성 때문에 많이 사용되는데요, 크게 두 가지 단점이 있습니다. 
첫 번째는 그룹을 나누기 위해 데이터를 mapping하는 공간이 **원본 데이터 공간에 한정**되어 있다는 것입니다.
두 번째로는 원본 데이터 공간에 mapping하는 특성 때문에 원본 데이터의 차원이 높아질수록 **연산상의 효율이 매우 떨어진다**는 점입니다.


물론 이를 보완하고자 하는 방법론이 제안된 적이 있습니다. 한 가지 예시로 큰 차원의 데이터로 인해 발생하는 비효율을 방지하기 위해 K-Means 클러스터링을 진행한 이후 낮은 차원으로 projection하는 방법론이 있습니다. 그러나 이 방법론은 projection이 linear embedding으로 제한된다는 한계가 존재합니다. 


또다른 예시로는 Spectral Clustering 이후 데이터를 임베딩하는 방법론이 있습니다. Spectral Clustering은 그래프 기반의 클러스터링 기법입니다. 간단하게 소개하자면 먼저 인접행렬(Adjacency Matrix)을 사용하여 데이터를 방향이 없는 그래프 형태로 변환합니다. 그리고 각 노드를 잇는 가중치가 가장 작은 부분을 잘라내어 군집을 구분하는 기법입니다. 많은 Spectral Clustering 류의 방법론에서, 가장 적합한 그래프 Cut 지점을 찾기 위해 목적함수를 최적화하는 과정에서 Full Laplacian Matrix를 계산해야 합니다. 그렇기 때문에 데이터의 크기가 클수록 연산의 효율이 매우 떨어진다는 한계점이 있습니다.


## Feature Space of Data

대부분의 이전 클러스터링 방법론은 우선 데이터를 특징 공간에 mapping한 이후 군집을 할당하는 과정을 진행합니다.
그러나 본 논문에서는 초점을 조금 돌려서, 클러스터링이 잘 수행될 수 있는 Feature space(*"Representations for clustering"*)를 찾는 것에도 집중하였습니다. 그러기 위해서는 최적의 군집을 찾기 위해 원본 데이터의 공간이 아닌, **데이터의 핵심적인 특징이 잘 반영되는 공간**을 찾을 필요성이 있었습니다.


논문이 나온 당시(2016년)에도 심층 신경망으로 데이터의 핵심적인 특징을 추출하는 연구가 진행되고 있었습니다. 그러나 이들은 label이 존재하는 지도학습 기반의 방법론들입니다. 본 논문에서는 비지도학습으로 특징을 추출하는 것을 목표로 했기 때문에 결이 약간 다르다고 할 수 있습니다.


## Best Feature + Clustering = ?

앞서 언급한 기존 클러스터링 기법의 한계점을 정리하면 아래와 같습니다.

- 원본 데이터의 공간에 mapping하기 때문에 차원이 커질수록 비효율적임
- 낮은 차원으로 축소하려는 시도가 있지만, linear embedding으로 한정되거나 데이터가 클수록 연산량이 급격히 증가함
  

그리고 저자들이 고민한 방향은 아래와 같습니다.

- '비지도학습'으로 최적의 군집을 형성하는 Feature Space를 찾으면서 클러스터링까지 수행하는 방법


위 고민들을 해결하기 위해 다음과 같은 방법을 제시합니다. 먼저 원본 데이터(X)의 공간에서 저차원의 특징(Z) 공간으로 mapping하는 non-linear 함수를 정의합니다. 그리고 해당 공간에서 클러스터링을 위한 목적함수를 최적화합니다. 이 알고리즘을 **Deep Embedded Clustering(DEC)** 라 명명합니다.


그러나 매우 짧게 소개한 DEC 알고리즘은 몇 가지 난관이 존재합니다. 먼저 별도의 label이 존재하지 않는다는 점이 최적화를 어렵게 만듭니다. 저자들은 이 문제를 해결하기 위해 아래와 같은 기법을 도입하여 반복적으로 최적의 클러스터를 찾습니다. 아래의 기법에 대해서는 뒤에서 자세히 설명하겠습니다.

- Soft Cluster Assignment
- Auxiliary Target Distribution


또한 고차원 상의 데이터를 저차원으로 mapping할 때 주의해야 할 점은 두 공간(원 데이터의 공간, 특징 공간)에서의 분포 간 거리가 유사해야 한다는 것입니다. t-SNE 방법론에서도 언급되는데, 두 공간에서의 분포를 가깝게 하기 위해서는 KL Divergence를 0에 가깝도록 최적화해야 합니다. 본 논문에서는 약간의 변형을 가하여 **centroid-based distribution과 (원 데이터의 공간 상의 분포 대신) auxiliary target distribution의 KL Divergence를 최소화**하고자 합니다. 이렇게 할 경우 연산 복잡도가 $O(nk)$까지 감소한다고 합니다. 참고로 $k$는 centroid의 개수이며 자세한 내용은 뒤에서 설명하겠습니다.


# [2] 방법론

## Deep Embedded Clustering (DEC)

DEC 알고리즘은 아래 두 개의 항목을 동시에 학습합니다.

  1. Feature space $Z$에 존재하는 $k$개 군집의 중심:  $  \{ \mu_j \in Z \}_{j=1}^k  $
  2. 원 데이터를 저차원 공간 $Z$로 mapping하는 신경망의 파라미터 $\theta$


학습은 두 단계를 통해 진행되는데 각 단계에 대한 설명은 아래와 같습니다.


### 1. Clustering with KL Divergence
DEC의 가장 핵심적인 클러스터링 학습 과정에 대해 먼저 설명하겠습니다. 먼저 초기 non-linear mapping에 대한 함수 $f_{\theta}$의 파라미터와 군집의 중심(centroid)  $  \{ \mu_j \}_{j=1}^k  $ 가 주어졌다고 가정합니다(이 부분은 다음 단계에서 설명합니다). 
이후 아래와 같은 단계로 클러스터링을 진행합니다. 


**① Soft Assignment** <br>

t-SNE 방법론에 대한 설명을 잠깐 언급하겠습니다. Stochastic Neighbor Embedding 방법론에 따르면, 저차원 공간에서 $j$번째 샘플이 $i$번째 샘플의 이웃이 될 확률($q_{j \mid i}$)을 계산할 때 가우시안 분포를 따른다고 가정할 경우 **Crowding Problem**이 발생합니다. Crowding Problem은 가우시안 분포의 평균에서 적당한 거리를 이동했을 때 확률밀도가 급격하게 작아지는 현상을 의미합니다. 이를 방지하기 위해 가우시안 분포보다 꼬리 부분이 두꺼운 t-분포를 사용합니다. 관련해서는 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=INHwh8k4XhM&list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&index=8)를 참고하시면 좋을 것 같습니다.


본 논문에서는 저차원 공간에 임베딩된 샘플 $z_i$와 군집의 중심 $\mu_j$ 간의 유사도(similarity)를 측정할 때 t-SNE 방법론의 설정을 따라 t-분포를 사용합니다. 따라서 <b>$i$번째 샘플이 $j$번째 군집에 할당될 확률 $q_{ij}$ </b>는 아래와 같습니다.

$$ q_{ij} = { {(1+\lVert z_i-\mu_j \rVert^2 / \alpha )^{-{ \alpha+1}\over{2} } } \over { \sum_{j^\prime} {(1+\lVert z_i-\mu_{j^\prime} \rVert^2 / \alpha )^{- { \alpha+1}\over{2} }} } } $$


여기서 $\alpha$는 t-분포의 자유도인데, 본 논문의 task에서 비지도학습을 가정하여 자유도에 대한 검증을 할 수 없고 학습에 불필요하기 때문에 모든 실험에서 $\alpha$를 1로 설정합니다. 이렇게 구한 $q_{ij}$가 바로 **Soft Assignment** 입니다. 군집을 할당하는 과정을 확률적으로 진행하기 때문에 Soft라는 명칭이 붙은 것 같습니다.


**② KL Divergence Minimization** <br>

앞선 단계에서는 초기화된 임베딩 공간 내에서 특정 군집에 할당될 확률($q_i$)을 계산했습니다. 원래대로라면 이 분포와 원 데이터 공간에서의 분포 간의 KL Divergence를 최소화해야 합니다. 하지만 본 논문에서는 <b>Auxiliary target distribution($p_i$)</b>이라는 개념을 도입하여 원 데이터 공간 상 분포와의 비교가 아닌, 같은 임베딩 공간 내에서 임의의 타겟 분포를 비교합니다. 즉 두 분포($p_i, q_i$)의 KL Divergence를 최소화하는 것을 목적함수로 삼는 것입니다. 식으로 표현하면 아래와 같습니다.

$$ L=KL(P\parallel Q)=\sum_i \sum_j p_{ij} \log {p_{ij} \over q_{ij}} $$


따라서 목적함수와 직결되는 target distribution $P$를 신중하게 선택해야 합니다. 논문의 저자는 target distribution이 만족해야 할 조건을 다음과 같이 제시합니다. 

- 예측의 확실성을 높여 클러스터링의 구분이 더 잘 되어야 함
- 높은 confidence로 군집이 할당된 데이터를 강조해야 함
- 각 중심이 loss에 기여하는 정도를 normalize하여 커다란 군집이 feature space를 왜곡하지 않아야 함



그리고 $p_i$를 아래와 같이 제시하여 위의 조건을 만족하도록 합니다. 개인적으로는 논문에서 $p_i$를 설정한 과정에 대해 조금 더 상세하게 서술했으면 하는 아쉬움이 있습니다.

$$ p_{ij} = { {q_{ij}^2 / f_j } \over {\sum_{j^\prime} q_{i{j\prime}}^2 / f_{j\prime} }  } \;\;\; (f_j = \sum_i q_{ij}) $$


**③ Optimization** <br>
군집의 중심 $ \{ \mu_j \} $와 신경망의 파라미터 $\theta$를 joint하게 최적화하기 위해서는 Stochastic Gradient Descent(SGD) + Momentum을 사용하였습니다. 
위에서 계산된 Loss는 임베딩 공간 상의 데이터 $z_i$와 중심점 $\mu_j$에 대해 편미분될 것이고, 각각에 대한 편미분값은 아래와 같습니다.

$$ {\partial L \over \partial z_i} = {\alpha +1 \over \alpha} \sum_j (1+{\lVert z_i-\mu_{j} \rVert^2 \over \alpha} )^{-1} \times (p_{ij}-q_{ij})(z_i-\mu_j) $$


$$ {\partial L \over \partial \mu_j} = -{\alpha +1 \over \alpha} \sum_i (1+{\lVert z_i-\mu_{j} \rVert^2 \over \alpha} )^{-1} \times (p_{ij}-q_{ij})(z_i-\mu_j) $$


편미분값 $\partial L / \partial z_i$는 신경망으로 전달되어 일반적인 역전파 방식으로 신경망의 파라미터에 대한 편미분값 $\partial L / \partial \theta$를 계산하게 됩니다. 이렇게 총 세 단계로 구분되는 클러스터링 과정은 수렴 조건을 만족할 때까지 반복됩니다. 수렴 조건은 연이은 두 번의 반복과정에서 군집 할당에 대한 변화량이 $tol\%$ 보다 작을 때이고, 만족할 경우 학습을 중단합니다.


### 2. Parameter Initialization with a Deep AutoEncoder

지금까지 언급한 클러스터링 과정은 신경망의 초기 파라미터 $\theta$와 군집의 중심(centroid) $\{ \mu_j \}$가 주어졌다는 가정 하에 진행됩니다. 이제 이 파라미터와 군집의 중심이 어떻게 초기화되는지 설명하겠습니다.


원 데이터에 대한 의미있는 특징을 추출하기 위해 Stacked AutoEncoder(SAE)로 DEC 구조를 초기화합니다. 식으로 구조를 나타내면 아래와 같습니다. 

$$ \tilde{x} \sim Dropout(x) $$ 

$$ h= g_1(W_1 \tilde{x} + b_1)  $$ 

$$ \tilde{h} \sim Dropout(h) $$ 

$$ y= g_2(W_2 \tilde{h} + b_2)  $$ 


해당 구조에서 모델 파라미터는 $ \{ W_1, b_1, W_2, b_2  \} $이고, $g_1$과 $g_2$는 활성화함수입니다.
학습은 입력값과 출력값의 Least-square loss인 $ \lVert x-y \rVert ^2_2  $를 최소화하는 방향으로 진행됩니다.
학습 이후 AE의 디코더 부분은 사용하지 않고 인코더만을 가져와서 초기 임베딩 공간을 형성합니다.


군집의 중심을 초기화하는 방법은 다음과 같습니다. 입력 데이터를 초기화된 신경망에 넣은 후 임베딩된 feature를 대상으로 K-Means 클러스터링을 진행합니다. 그 결과로 $k$개의 초기 군집 중심을 얻을 수 있습니다. 아래 그림은 DEC 알고리즘의 전체적인 구조입니다.


<img src="/assets/figures/dec_structure.PNG" width="70%">


# [3] 실험

본 논문에서는 두 개의 이미지 데이터셋(MNIST, STL-10)과 하나의 텍스트 데이터셋(REUTERS)에 대해 실험을 진행하였습니다. 또한 DEC 방법론에 대한 비교군으로 일반적인 K-Means 클러스터링과 Spectral 클러스터링 기반의 LDGMI, SEC 알고리즘을 설정하였습니다.


성능 검증을 위해서 군집의 개수는 실제(ground-truth) 카테고리의 개수와 동일하게 설정하였습니다. 평가 지표로는 Unsupervised Clustering Accuracy(ACC)를 사용합니다. 식으로 표현하면 아래와 같습니다. $l_i$는 실제 카테고리, $c_i$는 알고리즘에 의해 할당된 군집, 그리고 $m$은 군집과 실제 카테고리를 1대 1로 mapping하는 것을 의미합니다.

$$ ACC= \max_m { \sum_{i=1}^n \mathbf{1} \{ l_i=m(c_i) \} \over n }  $$


## 실험 결과

저자들은 정량적, 정성적인 실험 결과를 모두 확인하였습니다. 

### 정량적인 확인

먼저 실험에 대한 정량적인 지표는 아래와 같습니다. 총 9개의 하이퍼파라미터 조합에 대해 각각 실험을 진행했는데요, DEC 알고리즘의 성능이 좋으면서도 상대적으로 하이퍼파라미터 설정에 robust하다는 것을 확인할 수 있습니다.

<img src="/assets/figures/dec_exp.PNG" width="70%">

<img src="/assets/figures/dec_table.PNG" width="70%">


비교 모델 중 "DEC w/o backprop"이 있는데요, 이는 DEC 알고리즘의 end-to-end 효과를 검증해보기 위해 신경망의 파라미터 $\theta$를 업데이트하지 않은 모델을 의미합니다. 전반적으로 일반 DEC보다는 성능이 낮은 모습을 보입니다.


위 표에서는 나와 있지 않지만 또 한 가지 주목해야 할 점은, DEC의 속도가 매우 빠르다는 것입니다. REUTERS 데이터셋에 대해 학습할 때, DEC는 GPU를 사용하여 30분 정도가 걸렸다고 합니다. 그런데 Spectral 클러스터링 기반의 LDGMI와 SEC는 한 달 이상 걸리고, 테라바이트 단위의 메모리가 필요하다고 합니다. 그래서 저자들도 REUTERS 데이터셋에 대해서는 LDGMI와 SEC 알고리즘을 적용하지 못했습니다.

### 정성적인 확인

실험 결과를 정성적으로 살펴보기 위해 MNIST와 STL-10 데이터셋에 대해 각 군집마다 가장 높은 스코어를 갖는 이미지 10개씩을 출력하였습니다. 결과는 아래와 같습니다. 약간의 오답을 제외하고는 실제 카테고리와 흡사한 모습을 볼 수 있습니다.

<img src="/assets/figures/dec_img.PNG" width="70%">


## Discussion


### 1. 기본 가정과 목적함수에 대한 검증

DEC의 기본 가정은 초기 분류의 예측이 대부분 정확하다는 것입니다. 이 가정과 선택한 target distribution $P$가 적절한지를 검증하였습니다. MNIST 데이터셋 내 무작위 군집 $j$에 대해 계산된 Loss를 임베딩 공간의 샘플 $z_i$로 편미분한 값과 soft assignment $q_{ij}$를 plot으로 출력했을 때 아래와 같은 결과가 나옵니다.

<img src="/assets/figures/dec_plot.PNG" width="70%">

군집의 중심에 가까워질수록($q_{ij}$가 커질수록) gradient에 기여하는 정도가 큰 것을 확인할 수 있습니다. 출력된 이미지 역시 오른쪽으로 갈수록 카테고리에 부합하는 이미지인 것이 보입니다.

### 2. 반복적인 최적화의 기여도에 대한 검증

반복적으로 군집의 중심을 조정하는 것이 실제로 효과가 있는지에 대한 검증을 하였습니다. 역시 MNIST 데이터로 진행하였고, 임베딩된 데이터에 대해 t-SNE로 시각화를 한 결과는 아래와 같습니다.

<img src="/assets/figures/dec_iter.PNG" width="70%">

12번의 반복까지는 군집이 점점 더 뚜렷하게 형성되는 것을 확인할 수 있습니다.

### 3. AutoEncoder 초기화에 대한 검증

실험 결과 부분에서 사용된 방법론은 클러스터링을 진행한 샘플의 차원이 다릅니다. 
본 검증에서는 AutoEncoder로 추출한 feature에 대해 모든 기법을 적용하여 비교하였습니다.
결과는 아래와 같습니다.

<img src="/assets/figures/dec_ae.PNG" width="70%">

추출된 feature로 클러스터링을 진행하였을 때 성능 향상을 확인할 수 있습니다. SEC와 LDMGI의 성능 변화는 거의 없지만 K-Means는 성능 향상의 폭이 상대적으로 큽니다.


### 4. Imbalanced Data에 대한 성능 검증

불균형 데이터에 대해서도 잘 동작하는지를 확인하기 위해 MNIST 데이터에서 각 카테고리의 비율을 조정하며 실험하였습니다. 결과는 아래와 같습니다.

<img src="/assets/figures/dec_imb.PNG" width="70%">


$r_{min}$이 작아질수록 불균형이 심해지는 것인데, DEC가 다른 방법론에 비해 robust한 변화를 보이고 있음을 확인할 수 있습니다.

### 5. 적절한 군집 개수에 대한 검증

사실 지금까지의 실험에서는 군집의 개수를 실제 카테고리의 개수에 맞추어 설정했습니다. 그러나 실제 비지도학습 상황에서 우리는 카테고리의 개수를 알 수 없습니다. 따라서 최적의 군집 개수를 찾는 방법이 필요합니다.


이에 저자들은 다음 두 가지 척도를 제시합니다. 첫 번째는 Normalized Mutual Information(NMI)입니다. 주어진 군집의 개수에서 얻을 수 있는 정보량을 의미합니다. 식으로 나타내면 아래와 같습니다. $I$는 mutual information metric이고, $H$는 엔트로피입니다.

$$ NMI(l,c)= { {I(l,c)} \over  {1\over 2} [H(l)+H(c)] } $$

두 번째 척도는 Generalizability($G$) 입니다. 이는 training loss와 validation loss의 비율로 정의되는데, 식으로 나타내면 아래와 같습니다. G가 작아진다는 것은 training loss가 validation loss보다 작아지는 것을 의미하기 때문에 오버피팅을 의심할 수 있습니다.

$$ G={L_{train} \over {L_{validation}}} $$


MNIST 데이터에서 두 가지 척도에 대해 최적의 군집 개수를 찾은 결과는 아래와 같습니다.
 
<img src="/assets/figures/dec_cluster.PNG" width="70%">

군집의 개수가 9인 시점부터 $G$가 급격히 감소하고 $NMI$ 역시 더이상 증가하지 않는 것으로 보아 최적의 군집 개수는 9일 것입니다. MNIST 데이터셋의 카테고리는 10개인데, 9와 4의 형태가 유사하여 하나의 군집으로 묶었다고 유추할 수 있습니다.

# [4] 코드
논문의 내용을 참고하여 코드를 짜 보았습니다. 모든 환경을 정확하게 설정한 것이 아니라 논문의 성능과 차이가 있음을 미리 말씀드립니다. 전체 코드는 [여기](https://github.com/youngerous/dec-pytorch)에서 확인하실 수 있습니다. 학습은 아래와 같이 진행하였습니다. 방법론에 대한 설명은 위에서 하였기 때문에 코드 위주로 첨부하겠습니다.

### Dataset 
MNIST 데이터에 대해서 실험을 진행하였습니다.

```python
def prepare_data(self) -> None:
    transform = transforms.Compose([transforms.ToTensor(),])

    # Hyperparameter Tuning by cross-validation on a validation set is not an option in unsupervised clustering.
    # So train and test set are combined.
    train_dset = MNIST(os.getcwd(), train=True, transform=transform, download=True)
    test_dset = MNIST(os.getcwd(), train=False, transform=transform, download=True)
    self.dset = ConcatDataset([train_dset, test_dset])
```


### Pretraining & Finetuning Stacked AutoEncoder 
먼저 Linear 레이어로 구성된 Stacked AutoEncoder(SAE)를 학습하였습니다. MNIST 이미지를 벡터화하여 인코더를 통과하면 10차원의 feature가 생성됩니다. 그 후 본래 28*28차원의 벡터로 복원하는 디코더를 사용하여 기존 입력값과 비교합니다. Pretraining 시 인코더와 디코더의 형태는 아래와 같습니다.


<img src="/assets/figures/dec_encdec.png" width="70%">


학습은 본 벡터와 복원 벡터의 MSE를 줄이는 방식으로 진행됩니다. Pretraining 시 MSE는 0.22, Finetuning 시 MSE는 0.18까지 학습하였으며 모델 코드의 일부를 함께 첨부합니다.

```python
class SAE(pl.LightningModule):
    """
    Stacked AutoEncoder for pretraining the initial parameter and cluster centroid.

    :param dimensions: [input_dim, hidden_dim_1, ..., hidden_dim_N]
    :param activation: Non-linear activation function both in encoder and decoder
    :param final_activation: Non-linear activation function in final layer
    :param dropout: Dropout rate in each layer

    :param batch_size: Size of minibatch
    :param lr: Learning rate
    :param lr_decay: Learning rate decay ratio
    :param lr_decay_step: Learning rate decay frequency
    :param weight_decay: Weight decay ratio
    """

    def __init__(
        self,
        dimensions: Iterable[int],
        activation: Optional[nn.Module] = nn.ReLU(),
        final_activation: Optional[nn.Module] = None,
        dropout: Optional[float] = 0.0,
        batch_size: int = 256,
        lr: float = 0.1,
        lr_decay: float = 0.1,
        lr_decay_step: int = 20000,
        weight_decay: float = 0.0,
    ):
        super(SAE, self).__init__()
        self.criterion = nn.MSELoss()

        # stack encoder layers
        encoder_layers = self._add_linear_layer_stack(
            dimensions[:-1], activation, dropout
        )
        encoder_layers.extend(
            self._add_linear_layer_stack(
                [dimensions[-2], dimensions[-1]], final_activation, dropout=None,
            )
        )
        self.encoder = nn.Sequential(*encoder_layers)

        # stack decoder layers
        decoder_layers = self._add_linear_layer_stack(
            list(reversed(dimensions[1:])), activation, dropout
        )
        decoder_layers.extend(
            self._add_linear_layer_stack(
                [dimensions[1], dimensions[0]], final_activation, dropout=None,
            )
        )
        self.decoder = nn.Sequential(*decoder_layers)

        # initialize parameter
        self.encoder.apply(self._init_weight)
        self.decoder.apply(self._init_weight)

        self.save_hyperparameters()

    def forward(self, batch: torch.Tensor) -> torch.Tensor:
        encoded = self.encoder(batch)
        return self.decoder(encoded)

    def _add_linear_layer_stack(
        self,
        dims: Iterable[int],
        activation: Optional[nn.Module],
        dropout: Optional[float],
    ) -> List[nn.Module]:
        def single_unit(in_dim: int, out_dim: int) -> List[nn.Module]:
            unit = [nn.Linear(in_dim, out_dim)]
            if activation is not None:
                unit.append(activation)
            if dropout is not None:
                unit.append(nn.Dropout(0.2))
            return nn.Sequential(*unit)

        return [single_unit(dims[idx], dims[idx + 1]) for idx in range(len(dims) - 1)]
    
    def _init_weight(self, layer):
        if type(layer) == nn.Linear:
            nn.init.normal_(layer.weight, mean=0.0, std=0.01)  # follow paper setting
            nn.init.constant_(layer.bias, 0)
```

학습한 오토인코더로 샘플을 복원한 결과는 아래와 같습니다. 

<img src="/assets/figures/dec_ex.png" width="70%">


### Training Deep Embedded Clustering

학습한 인코더의 임베딩 벡터를 사용하여 새로운 클러스터링을 학습합니다. 비지도학습 기반이기 때문에 Soft Assignment와 Auxiliary Target Distribution을 정의하여 둘의 KL Divergence를 줄이는 방향으로 학습합니다. 아래 DEC 모델은 입력 벡터가 들어왔을 때 해당하는 soft assignment를 출력하는 구조입니다.

```python
class DEC(pl.LightningModule):
    """
    Deep Embedded Clustering

    :param encoder: Finetuned Encoder of Stacked AutoEncoder
    :param num_cluster: Number of cluster
    :param hidden_dim: Dimension of final encoder vector
    :param alpha: Freedom value of t-distribution
    :param batch_size: Batch size
    :param lr_dec: Learning rate
    :param tol: Threshold to stop training
    """

    def __init__(
        self,
        encoder: nn.Module,
        num_cluster: int = 10,
        hidden_dim: int = 10,
        alpha: float = 1.0,
        batch_size: int = 256,
        lr_dec: float = 0.01,
        tol: float = 1e-3,
    ):
        super(DEC, self).__init__()
        self.save_hyperparameters()
        self.encoder = encoder
        self.assignment = SoftClusterAssignment(num_cluster, hidden_dim, alpha)
        self.criterion = F.kl_div
        self.kmeans = KMeans(self.hparams.num_cluster, n_init=20)
        self.init = True

    def forward(self, batch):
        return self.assignment(self.encoder(batch))

```

Soft Assignment와 Target Distribution에 대한 코드는 아래와 같이 설정하였습니다.
```python
# Auxiliary Target Distribution
def _get_target_distribution(self, q):
    numerator = (q ** 2) / torch.sum(q, 0)
    p = (numerator.t() / torch.sum(numerator, 1)).t()
    return p

# Soft Assignment
class SoftClusterAssignment(nn.Module):
    def __init__(
        self,
        num_cluster: int,
        hidden_dim: int,
        alpha: float = 1.0,
        centroid: torch.tensor = None,
    ):
        super(SoftClusterAssignment, self).__init__()
        self.num_cluster = num_cluster
        self.hidden_dim = hidden_dim
        self.alpha = alpha

        if centroid is None:
            initial_centroid = torch.zeros(
                self.num_cluster, self.hidden_dim, dtype=torch.float
            )
            nn.init.normal_(initial_centroid, mean=0.0, std=0.01)
        else:
            initial_centroid = centroid
        self.centroid = initial_centroid.cuda()  ##

    def forward(self, z):
        z = z.cuda()
        diff = torch.sum((z.unsqueeze(1) - self.centroid) ** 2, 2)
        numerator = 1.0 / (1.0 + (diff / self.alpha))
        power = (self.alpha + 1.0) / 2
        numerator = numerator ** power
        q = numerator / torch.sum(numerator, dim=1, keepdim=True)
        return q

```

그리고 DEC를 처음 학습할 때에는 K-Means 클러스터링으로 초기 centroid를 할당해주어야 합니다. DEC 학습을 시작하기 직전에 아래의 코드로 초기화하였습니다.

```python
def _initialize_centroid(self) -> dict:
        print("Set Initial Centroid...")
        dloader = DataLoader(
            self.dset, batch_size=self.hparams.batch_size, shuffle=True, drop_last=True
        )
        label, feature = [], []

        for batch in dloader:
            data, target = batch
            data, target = data.to(self.device), target.to(self.device)
            label.append(target)
            feature.append(
                self.encoder(data.reshape(self.hparams.batch_size, -1)).detach().cpu()
            )
        label = torch.cat(label)
        pred = self.kmeans.fit_predict(torch.cat(feature).numpy())
        accuracy = cluster_acc(label.cpu().numpy(), pred)

        return {
            "accuracy": accuracy,
            "centroid": torch.tensor(
                self.kmeans.cluster_centers_, requires_grad=True
            ).cuda(),
        }
```

구현 코드에서의 성능 확인은 Unsupervised Clustering Accuracy로 진행하였습니다.
제 코드로는 논문의 성능에 한참 못미칩니다만(...), SAE로 학습한 임베딩 벡터의 초기 성능(약 57%)에서 비지도학습으로 73%까지 끌어올릴 수 있었습니다.

<img src="/assets/figures/dec_impl.png" width="70%">


# [5] 마치며

본 논문이 기여한 바를 정리하며 글을 마치겠습니다.
  - 임베딩과 클러스터링을 동시에 최적화하였음
  - Soft assignment를 통해 반복적으로 최적의 centroid를 탐색하였음
  - 클러스터링 정확도와 속도 측면에서 state-of-the-art 성능을 보였음


# [6] 참고자료
- [[Paper] Unsupervised Deep Embedding for Clustering Analysis](https://arxiv.org/abs/1511.06335)
- [[Post] Spectral Clustering](https://ratsgo.github.io/machine%20learning/2017/04/27/spectral/)
- [[Lecture] Dimensionality Reduction: t-SNE](https://www.youtube.com/watch?v=INHwh8k4XhM&list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&index=8)
- [[Implementation Code] dec-pytorch](https://github.com/youngerous/dec-pytorch)